{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TsPR9wc5_70_isxcjOj-QL-eBYwzwjqA",
      "authorship_tag": "ABX9TyObFAAEXTy1A9P/KeTzufow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gguex/ISH_ressources_cours_ML/blob/main/TP11_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP 11 : LSTM et GRU\n",
        "\n",
        "Dans ce TP, nous allons apprendre à créer et entrainer des réseaux de type LSTM et GRU pour créer des modèles génératifs.\n",
        "\n",
        "Dans la première partie, nous allons entrainer un réseau LSTM qui permet de générer du texte \"similaire\" à des données d'entrainement. Nous utiliserons pour cela le premier livre des Misérables.\n",
        "\n",
        "Dans la deuxième partie, nous allons entrainer un réseau GRU qui permet de générer des prénoms féminins ou masculins qui \"sonnent juste\", mais qui n'existent pas nécessairement. Nous utiliserons pour cela le jeu de données de prénoms anglophones trouvé sur http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/ (mais vous pouvez essayer avec d'autres listes de prénoms, ces dernières sont faciles à trouver).\n",
        "\n",
        "Les librairies nécessaires sont les suivantes :"
      ],
      "metadata": {
        "id": "xJftcrf_fBjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqoWHussKYEq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "import string\n",
        "# Permet d'afficher le texte avec une certaine largeur\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enregistrons le dispositif de calcul dans une variable."
      ],
      "metadata": {
        "id": "S8FXdFVtj3Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ksaGk3fkEX-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5ecbb7-0ec2-4dd8-b37e-1f6fd7fc7ba7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Génération de textes avec LSTM\n",
        "\n",
        "Dans cette partie, nous allons créer un modèle de génération de textes qui sera entrainé sur le premier livre des Misérables (cf. TP 10). Ce modèle sera constitué de plusieurs couches LSTM successives, et devra être entrainé à prédire le prochain token d'une séquence en fonction des tokens précédents."
      ],
      "metadata": {
        "id": "eXUpfBThkE4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` pour pouvoir générer des exemples d'entrainement.\n",
        "\n",
        "Un entrée/sortie de ce dataset sera constitué de deux séquences, tirées de notre document, de taille `seq_len`. La séquence de sortie sera décalée d'un token sur la droite par rapport à l'entrée (voir l'exemple plus bas). Ainsi, chaque token de la séquence d'entrée sera associé avec le token suivant.\n",
        "\n",
        "Nous allons utiliser un modèle de langage de Spacy pour tokeniser notre document."
      ],
      "metadata": {
        "id": "y0ATSP1nku7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceData(Dataset):\n",
        "    def __init__(self, file_path, nlp, seq_len):\n",
        "\n",
        "        # Le chemin d'accès au fichier\n",
        "        self.file_path = file_path\n",
        "        # La longueur des séquences d'entrée et sortie\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # On ouvre notre fichier et on crée un object Spacy\n",
        "        with open(self.file_path, \"r\") as f:\n",
        "          text = f.read()\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # On découpe notre texte en tokens, sans les espaces\n",
        "        self.tokens = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "        # On enregistre le vocabulaire utilisé, et on crée des dictionnaires\n",
        "        # permettant de transformer chaque token en identifiant numérique,\n",
        "        # ou l'inverse.\n",
        "        self.dictionary = list(set(self.tokens))\n",
        "        self.id2token = {id: token for id, token in enumerate(self.dictionary)}\n",
        "        self.token2id = {token: id for id, token in enumerate(self.dictionary)}\n",
        "\n",
        "        # On transforme notre corpus en une séquence de valeurs numériques\n",
        "        self.token_ids = [self.token2id[w] for w in self.tokens]\n",
        "\n",
        "    # La taille de notre dataset est le nombre de séquences possibles\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.seq_len\n",
        "\n",
        "    # Un élément de notre corpus sera la séquence id et la séquence id+1.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.token_ids[id:id+self.seq_len]),\n",
        "                torch.tensor(self.token_ids[id+1:id+self.seq_len+1]))"
      ],
      "metadata": {
        "id": "aBf2WJTKEXEO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On charge le modèle de langage de Spacy."
      ],
      "metadata": {
        "id": "l4tegHb-oKGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !spacy download \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "nX5JqfXwEjyR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre classe `SentenceData`, en utilisant le premier livre de notre corpus et en le divisant en séquences de 10 tokens."
      ],
      "metadata": {
        "id": "HoC3igPr0MuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/book_01.txt\"\n",
        "example_doc = SentenceData(doc_path, nlp, 10)"
      ],
      "metadata": {
        "id": "4vRKSDCwGmnp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre document contient 28932 paires de séquences."
      ],
      "metadata": {
        "id": "51HAOH7u0VD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJMYd_sH1XI",
        "outputId": "53413bb2-8914-48a2-94c9-3138cdd747c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28932"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut examiner un exemple en particulier, en traduisant les séquences numériques en mots."
      ],
      "metadata": {
        "id": "RX4XIx9v0dpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id = 0\n",
        "input, output = example_doc[sample_id]\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in input]))\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in output]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBb9EO8XHORw",
        "outputId": "6420e04f-a4bf-40b2-b028-d2d9b2614aa9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 1815 , M. Charles - François - Bienvenu Myriel\n",
            "1815 , M. Charles - François - Bienvenu Myriel was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un objet `torch.utils.data.Dataloader` pour parcourir notre base de données, avec une taille de batch donnée."
      ],
      "metadata": {
        "id": "lXIhKhPm0nPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "my_dataloader = DataLoader(example_doc, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rP1RISBZNbzX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps de créer notre modèle. Ce dernier prendra notre séquence d'entrée et sera consititué de :\n",
        "\n",
        "* Une couche d'embedding, qui transforme nos identifiants numériques en vecteurs one-hot de taille `n_vocab`, puis en vecteur de taille `embedding_dim`.\n",
        "* `num_layers` couches de LSTM, avec des vecteurs d'état caché et d'état de cellule de taille `hidden_size`. On peut ajouter un dropout aux couches, afin que ces dernières n'apprennent pas le texte entièrement par coeur.\n",
        "* Une couche entièrement connectée, avec `n_vocab` sorties.\n",
        "\n",
        "La sortie de ce réseau donnera les log-odds pour les mots suivants, qui peuvent entre converties en probabilités.\n",
        "\n",
        "Notez que la longueur de la séquence n'a pas besoin d'être précisée ici. Pytorch se chargera de prendre tous les éléments reçus, de \"déplier\" le réseau en fonction de leur nombre, et de calculer les sorties en transmettant les états entre les cellules.\n",
        "\n",
        "Remarquez également que nous précisons avec `batch_first=True`, lors de la création des couches LSTM, que notre batch se situe sur la première dimension de nos tenseurs. Ainsi, Pytorch comprend que la séquence se situe sur la deuxième dimension des tenseurs (voir https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)."
      ],
      "metadata": {
        "id": "cm42-xGY1GmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqGen(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, hidden_size, num_layers):\n",
        "        super(SeqGen, self).__init__()\n",
        "\n",
        "        # On sauve les paramètres dans des attributs\n",
        "        self.n_vocab = n_vocab\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # La couche d'embedding\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        # Les couches LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    # La fonction foward peut prendre, en plus des entrées,\n",
        "    # l'état caché et l'état de la cellule utilisé au début de la séquence.\n",
        "    # Par défaut, ces états sont posés comme None.\n",
        "    def forward(self, x, prev_state=None):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "kj65kz_oKnr6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre modèle, avec nombre de mots correspondant à notre vocabulaire, embedding de 128 et 3 couches de LSTM avec états de taille 128."
      ],
      "metadata": {
        "id": "-YhjGZK7_5PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_gen = SeqGen(len(example_doc.dictionary), 128, 128, 3)"
      ],
      "metadata": {
        "id": "bcnKUn8oTO6e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser l'entropie croisée et l'optimisateur Adam pour entrainer notre modèle."
      ],
      "metadata": {
        "id": "LgnEYlT5AQTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(seq_gen.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jPcpxj7OT4VA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On entraine maintenant le modèle. Notez qu'aucun état caché est donné à notre modèle avant chaque batch, ce qui veut dire que les états initiaux seront nuls avant chaque séquence. L'entrainement est relativement simple, car la surparamétrisation n'est pas très importante pour notre modèle."
      ],
      "metadata": {
        "id": "B2581VwZAcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le nombre d'epochs\n",
        "n_epochs = 30\n",
        "\n",
        "# On met le modèle sur le dispositif de calcul\n",
        "seq_gen.to(device)\n",
        "# On le met en mode entrainement\n",
        "seq_gen.train()\n",
        "# On boucle sur les epochs\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "  # Pour calculer la perte moyenne\n",
        "  sum_loss = 0\n",
        "  # On boucle sur notre dataloader\n",
        "  for input, output in my_dataloader:\n",
        "\n",
        "    # Les entrées et sorties sont mises sur le dispositif de calcul\n",
        "    input = input.to(device)\n",
        "    output = output.to(device)\n",
        "\n",
        "    # On met à zéro les gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # On fait les prédictions\n",
        "    pred, _ = seq_gen(input)\n",
        "    # On calcule la perte, en transposant nos résultats\n",
        "    loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "    # On fait une itération de descente du gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # On ajoute la perte\n",
        "    sum_loss += loss.item()\n",
        "\n",
        "  print(f\"mean loss = {sum_loss / len(my_dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWiGXq-T7ty",
        "outputId": "f126b13a-04fa-4ee2-a0e6-8eb1ca80f8e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean loss = 6.7487\n",
            "Epoch 2: mean loss = 6.2715\n",
            "Epoch 3: mean loss = 6.1804\n",
            "Epoch 4: mean loss = 6.0350\n",
            "Epoch 5: mean loss = 5.8529\n",
            "Epoch 6: mean loss = 5.6564\n",
            "Epoch 7: mean loss = 5.4718\n",
            "Epoch 8: mean loss = 5.3008\n",
            "Epoch 9: mean loss = 5.1350\n",
            "Epoch 10: mean loss = 4.9768\n",
            "Epoch 11: mean loss = 4.8290\n",
            "Epoch 12: mean loss = 4.6939\n",
            "Epoch 13: mean loss = 4.5696\n",
            "Epoch 14: mean loss = 4.4485\n",
            "Epoch 15: mean loss = 4.3354\n",
            "Epoch 16: mean loss = 4.2326\n",
            "Epoch 17: mean loss = 4.1389\n",
            "Epoch 18: mean loss = 4.0545\n",
            "Epoch 19: mean loss = 3.9665\n",
            "Epoch 20: mean loss = 3.8858\n",
            "Epoch 21: mean loss = 3.8075\n",
            "Epoch 22: mean loss = 3.7356\n",
            "Epoch 23: mean loss = 3.6717\n",
            "Epoch 24: mean loss = 3.6004\n",
            "Epoch 25: mean loss = 3.5305\n",
            "Epoch 26: mean loss = 3.4642\n",
            "Epoch 27: mean loss = 3.3958\n",
            "Epoch 28: mean loss = 3.3308\n",
            "Epoch 29: mean loss = 3.2676\n",
            "Epoch 30: mean loss = 3.2095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant donner à notre modèle un début de séquence et voir comment il génère une suite. Le processus se passe en deux étapes :\n",
        "\n",
        "* On passe une première fois la séquence initiale dans notre modèle, afin de générer le prochain token et pour récupérer l'état caché résultant.\n",
        "* On va ensuite faire une boucle pour les tokens restants, en passant dans le réseau le dernier token et états obtenus à l'étape précédente.\n",
        "\n",
        "Pour générer chaque nouveau token, on transforme les log-odds en probabilités, puis on effectue un tirage aléatoire selon ces dernières.\n",
        "\n",
        "Le séquence finale est affichée formattée."
      ],
      "metadata": {
        "id": "P2pM7sEdGYaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Séquence initiale\n",
        "input_sentence = \"When the man\"\n",
        "# Nombre de tokens désiré\n",
        "n_generated_tokens = 600\n",
        "\n",
        "# --- Etape 1\n",
        "\n",
        "# On coupe notre séquence intiale pour créer notre séquence de sortie\n",
        "output_tokens = input_sentence.split()\n",
        "# On met le modèle en mode évaluation\n",
        "seq_gen.eval()\n",
        "# Notre séquence d'entrée est transformée en identifiants numériques\n",
        "input = torch.tensor([example_doc.token2id[token]\n",
        "                      for token in output_tokens]).to(device)\n",
        "# On passe nos entrées dans notre modèle, en ne donnant aucun état.\n",
        "pred, hidden = seq_gen(input)\n",
        "# On garde les logodds du dernier mot. Notez que nous utilisons\n",
        "# detach().cpu() pour ne pas garder le graph de calcul et pour\n",
        "# basculer le tenseur sur le cpu.\n",
        "new_token_logodds = pred.detach().cpu()[-1]\n",
        "# Les probabilités sont calculées avec un softmax\n",
        "probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "# On tire aléatoirement le token suivant, avec les probabilités précédentes\n",
        "token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "# On l'ajoute à notre sortie\n",
        "output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# --- Etape 2\n",
        "\n",
        "# On boucle sur le nombre restant de tokens demandés\n",
        "for i in range(n_generated_tokens - 1):\n",
        "  # Notre dernier token est transformé en entrée\n",
        "  input = torch.tensor([example_doc.token2id[output_tokens[-1]]]).to(device)\n",
        "  # On passe notre entrée, avec les états précédants, dans notre modèle\n",
        "  pred, hidden = seq_gen(input, hidden)\n",
        "  # On détache, met sur le cpu et applatit la sortie\n",
        "  new_token_logodds = pred.detach().cpu().flatten()\n",
        "  # Les probabilités sont calculées avec un softmax\n",
        "  probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "  # On tire aléatoirement, avec les probabilités calculées, le token suivant\n",
        "  token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "  # On l'ajoute à notre sortie\n",
        "  output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# On affiche notre sortie formatée\n",
        "print(textwrap.fill(\" \".join(output_tokens), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0JzgJA9VfIk",
        "outputId": "7b2cfb10-f6e2-4d16-d549-b6506d29c7d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the man is , the midst of no ignorant about other time , that had still be\n",
            "neither by his gallican . These variety which are a peasants for a carriage ,\n",
            "which is not only a magnificent revere with only lucky , and and ten -\n",
            "philosophy of those three man . Win which are single HOUSEHOLD with that\n",
            "harshness , which is forced to be a little man , he than he respected no hall\n",
            "was avoided . ” This girls , as in rusticity with nonsense , without Brignolles\n",
            "desirable , create from unity into indulgent during carriage , so , with its\n",
            "dangerous detested , very widely existed , he gave through in his old mitres .\n",
            "Each of the drawing - room of his man ? ” “ That is interrogated from the Duke\n",
            "are of these - peasants ; he can feel bear somewhat working - kill men , and\n",
            "becomes be expressing in walks Bienvenu ; there is nothing for its atheist at\n",
            "the outset appeared for sixty audaciously two two hours of our conversation\n",
            "which pleased that is its house ; probably strange of escape of pale ,\n",
            "everywhere we have none slowly , which is peculiar to matter , that he was\n",
            "yonder , still by what near that he angel settled on the despot , but the road\n",
            "if that Madame from not become a heart with his sister , them , speaking for\n",
            "two power , which slipped is neither in receipt of recognized,—that at great\n",
            "three wrote . This jurisdiction to anguish forth it , authorized that vaguely\n",
            "had attempts come at back , which may be amavit_,—because a sacred days for I\n",
            "time into pursued , so so also how to ’ call it delivery of going , from a\n",
            "demand , is up at him to wish , deceiving invented groups in an acre , beneath\n",
            "rather years is not a sacred men held alone upon which is at turn , Monsieur ,\n",
            "below , justice , at success , we come it takes , behold , than that from the\n",
            "hatred of France ; softened strange alone who was shoe - lys , and performs the\n",
            "olden agony , in short , above the midst of that gardens was a shadow of deeds\n",
            ", for its pastoral non accompanied presented for your heart for his regimentals\n",
            ", and far was long into his day . Gregory die , were made by his incurable\n",
            "which were understood , which was plump for its life , above the transformation\n",
            "the fate of His Dauphiné ; let it give to one persons ; as they often rising\n",
            "since but thirty - cart ; he knew nothing with liberty , even in chaplaincies ,\n",
            "as we most so - lines , was contemplation and Pascal , far from lucky , and to\n",
            "arouse those , without seeking to condense he gazed for him to be a worthy\n",
            "dreamed of bitterness , which Lucretius , oh for his things , and seeing an\n",
            "degree a cap hour of D — — _ I will be elsewhere . We analyzes on tolerable at\n",
            "the verge of Mousqueton misery , which seemed deciphering to mark ; his broad\n",
            "after grave reaction , the precipices of idioms . His little arm , then he had\n",
            "planted than — with his whole society which one story as on the stories which\n",
            "had stand formed but when also the first away which entered into being wounded\n",
            "in the intelligent waited , has there was its rascal ;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "weevWLMZ9bQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Génération de prénoms avec GRU\n",
        "\n",
        "Maintenant, nous allons utiliser les GRU pour créer des modèles de génération de prénoms, en utilisant des données de prénoms féminins et masculins. Le principe reste assez similaire à la partie précédente, mais nous adopterons une programmation de style plus fonctionnel."
      ],
      "metadata": {
        "id": "_zoqTHMJLtZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` qui va donner des exemples constitués d'un nom comme entrée et du même nom décalé d'une lettre sur la droite comme sortie. Comme caractère de fin de séquence et de padding, nous allons utiliser le caractère `\"\\n\"` déjà présent à la fin de chaque nom dans le fichier (sauf du dernier). Nous effectuons un padding sur tous les prénoms afin que tous fassent la taille du plus long prénom (pour entrainer les modèles avec des batchs)."
      ],
      "metadata": {
        "id": "qPy0pb8TMCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameData(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        # On enregistre le chemin du fichier\n",
        "        self.file_path = file_path\n",
        "        # On ouvre le fichier et on lit les lignes\n",
        "        with open(file_path, \"r\") as f:\n",
        "          self.lines = f.readlines()\n",
        "        # On ajoute notre caractère de padding à la dernière ligne\n",
        "        self.lines[-1] += \"\\n\"\n",
        "        # La longueur maximale est la longueur de la plus grande ligne\n",
        "        self.max_len = max([len(line) for line in self.lines])\n",
        "        # Les lignes sont complétées avec \"\\n\" pour avoir la même taille\n",
        "        self.lines = [line + \"\\n\"*(self.max_len - len(line))\n",
        "                      for line in self.lines]\n",
        "        # On met les lignes en minuscules\n",
        "        self.lines_low = [line.lower() for line in self.lines]\n",
        "        # On sauve le nombre de lettres différentes\n",
        "        self.letters = list(set(\"\".join(self.lines_low)))\n",
        "        # On crée notre liste de nom, avec chaque lettre\n",
        "        # comme élément d'une liste\n",
        "        self.names = [list(line) for line in self.lines_low]\n",
        "\n",
        "        # Ces fonctions permettent de transformer les lettres en identifiants\n",
        "        # numériques, ou vice-versa.\n",
        "        self.id2letter = {id: letter for id, letter in enumerate(self.letters)}\n",
        "        self.letter2id = {letter: id for id, letter in enumerate(self.letters)}\n",
        "\n",
        "        # On sauve l'identifiant du caractère de padding\n",
        "        self.endl_id = self.letter2id[\"\\n\"]\n",
        "\n",
        "        # On transforme nos listes de lettres en listes d'ids\n",
        "        self.names_ids = [[self.letter2id[l] for l in name]\n",
        "                          for name in self.names]\n",
        "\n",
        "    # La longueur du dataset\n",
        "    def __len__(self):\n",
        "        return len(self.names_ids)\n",
        "\n",
        "    # Un item est une entrée/sortie avec, respectivement,\n",
        "    # prénom sans la dernière lettre, prénom sans la première lettre.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.names_ids[id][:-1]),\n",
        "                torch.tensor(self.names_ids[id][1:]))"
      ],
      "metadata": {
        "id": "2PLVeGU39cC4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée maintenant nos instances contenant nos jeux de données."
      ],
      "metadata": {
        "id": "BpFQh1DkOrJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/female.txt\"\n",
        "male_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/male.txt\"\n",
        "female_data = NameData(female_path)\n",
        "male_data = NameData(male_path)"
      ],
      "metadata": {
        "id": "ukgFs2NC9yMT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons un exemple."
      ],
      "metadata": {
        "id": "9OLmhvMiO0ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, output = female_data[0]\n",
        "print([female_data.id2letter[id.item()] for id in input])\n",
        "print([female_data.id2letter[id.item()] for id in output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYDL5IPb94bZ",
        "outputId": "c1f7522f-4116-4432-c36a-f5c3a99c6a28"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n",
            "['b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous créons maintenant la classe de notre modèle. Ce dernier passe  directement les vecteurs one-hot dans les couches GRU, car le nombre de caractères existants n'est pas très élevé (la dimensionnalité des vecteurs one-hot est déjà basse). Le modèle est constitué de :\n",
        "\n",
        "* `num_layer` couches GRU avec états cachés de taille `hidden_size`.\n",
        "* Une couche entièrement connectée qui envoie les états cachés finaux sur la taille du vocabulaire.\n",
        "\n",
        "Le résultat du modèle sont les log-odds des prochains caractères."
      ],
      "metadata": {
        "id": "XE6CYho4jfOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameGen(nn.Module):\n",
        "    def __init__(self, n_vocab, hidden_size, num_layers):\n",
        "        super(NameGen, self).__init__()\n",
        "\n",
        "        # On enregistre les paramètres du modèle\n",
        "        self.n_vocab = n_vocab\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Les couches GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.n_vocab,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        # Cette fonction transforme les entiers en vecteurs one-hot\n",
        "        one_hot = nn.functional.one_hot(x, num_classes=self.n_vocab)\n",
        "        # On passe dans les couches GRU\n",
        "        output, state = self.gru(one_hot.to(torch.float), prev_state)\n",
        "        # Les log-odds résultantes\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "zQ0fLqLc-Eab"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous instancions un modèle et lui passons un exemple, pour voir si tout fonctionne."
      ],
      "metadata": {
        "id": "ZjaDToc6lQWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 20, 2)\n",
        "female_name_gen(input)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV5nAhek-FDs",
        "outputId": "ea40902b-5d4c-4423-9f51-2c6d6c6322e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'idée de prendre une approche fonctionnelle, nous définissons ici la fonction qui permet d'entrainer un modèle."
      ],
      "metadata": {
        "id": "idkOpfztle6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, batch_size, n_epochs, device):\n",
        "\n",
        "  # On définit la fonction de perte et l'optimisateur\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # On crée le dataloader\n",
        "  dataloader = DataLoader(data, batch_size, shuffle=True)\n",
        "\n",
        "  # On bascule le modèle en mode entrainement\n",
        "  model.train()\n",
        "  # On le met sur le dispositif de calcul\n",
        "  model.to(device)\n",
        "  # La boucle d'entrainement\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "    # Pour calculer la perte moyenne\n",
        "    sum_loss = 0\n",
        "    # La boucle sur les batchs\n",
        "    for input, output in dataloader:\n",
        "\n",
        "      # On met l'entrée et la sortie sur le dispositif de calcul\n",
        "      input = input.to(device)\n",
        "      output = output.to(device)\n",
        "\n",
        "      # On met les gradients à zéro, on fait des prédictions et on\n",
        "      # calcule la perte\n",
        "      optimizer.zero_grad()\n",
        "      pred, _ = model(input)\n",
        "      loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "      # On effectue la descente du gradient et on cumule les pertes\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item()\n",
        "\n",
        "    print(f\"mean Loss = {sum_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "s1xFZYsw-L1S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise maintenant la fonction créée pour entrainer un modèle avec les prénoms féminins."
      ],
      "metadata": {
        "id": "WU9xzgalmgBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 30, 3)\n",
        "train_model(female_name_gen, female_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUUZldXv-Q08",
        "outputId": "9887e781-5866-4f71-975f-093bd66fe106"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.3447\n",
            "Epoch 2: mean Loss = 0.9837\n",
            "Epoch 3: mean Loss = 0.9192\n",
            "Epoch 4: mean Loss = 0.8817\n",
            "Epoch 5: mean Loss = 0.8639\n",
            "Epoch 6: mean Loss = 0.8501\n",
            "Epoch 7: mean Loss = 0.8355\n",
            "Epoch 8: mean Loss = 0.8218\n",
            "Epoch 9: mean Loss = 0.8073\n",
            "Epoch 10: mean Loss = 0.7915\n",
            "Epoch 11: mean Loss = 0.7783\n",
            "Epoch 12: mean Loss = 0.7679\n",
            "Epoch 13: mean Loss = 0.7596\n",
            "Epoch 14: mean Loss = 0.7520\n",
            "Epoch 15: mean Loss = 0.7449\n",
            "Epoch 16: mean Loss = 0.7381\n",
            "Epoch 17: mean Loss = 0.7322\n",
            "Epoch 18: mean Loss = 0.7269\n",
            "Epoch 19: mean Loss = 0.7216\n",
            "Epoch 20: mean Loss = 0.7163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "eg4j_PXPmpOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_name_gen = NameGen(len(male_data.letters), 30, 3)\n",
        "train_model(male_name_gen, male_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PthBaFgw-TIH",
        "outputId": "c746685b-2c6c-4185-855e-4f0827ab7fb2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.5680\n",
            "Epoch 2: mean Loss = 1.1217\n",
            "Epoch 3: mean Loss = 1.0467\n",
            "Epoch 4: mean Loss = 1.0271\n",
            "Epoch 5: mean Loss = 1.0135\n",
            "Epoch 6: mean Loss = 0.9924\n",
            "Epoch 7: mean Loss = 0.9527\n",
            "Epoch 8: mean Loss = 0.9259\n",
            "Epoch 9: mean Loss = 0.9137\n",
            "Epoch 10: mean Loss = 0.9059\n",
            "Epoch 11: mean Loss = 0.8993\n",
            "Epoch 12: mean Loss = 0.8922\n",
            "Epoch 13: mean Loss = 0.8839\n",
            "Epoch 14: mean Loss = 0.8756\n",
            "Epoch 15: mean Loss = 0.8684\n",
            "Epoch 16: mean Loss = 0.8619\n",
            "Epoch 17: mean Loss = 0.8563\n",
            "Epoch 18: mean Loss = 0.8508\n",
            "Epoch 19: mean Loss = 0.8452\n",
            "Epoch 20: mean Loss = 0.8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons maintenant une fonction qui permet de générer un prénom en fonction des premiers caractères. La procédure est très similaire à la génération de tokens, mais on va arrêter la génération dès que notre modèle prédit le caractère \"\\n\", signifiant la fin de séquence."
      ],
      "metadata": {
        "id": "sepYp7C9snFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_name(init_letters, model, data, device):\n",
        "\n",
        "  # --- Etape 1\n",
        "\n",
        "  output_letters = list(init_letters)\n",
        "  model.eval()\n",
        "  input = torch.tensor([data.letter2id[letter]\n",
        "                        for letter in output_letters]).to(device)\n",
        "  pred, hidden = model(input)\n",
        "  new_letter_logodds = pred.detach().cpu()[-1]\n",
        "  probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "  next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "  output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  # --- Etape 2\n",
        "\n",
        "  # On boucle tant que l'on a pas généré \"\\n\"\n",
        "  while not output_letters[-1] == \"\\n\":\n",
        "    input = torch.tensor([data.letter2id[output_letters[-1]]]).to(device)\n",
        "    pred, hidden = model(input, hidden)\n",
        "    new_letter_logodds = pred.detach().cpu().flatten()\n",
        "    probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "    next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "    output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  return \"\".join(output_letters)[:-1]"
      ],
      "metadata": {
        "id": "m_YmXApw-WpE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testons la fonction de génération avec un début de prénom."
      ],
      "metadata": {
        "id": "X_cRCEZB72vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_name(\"lun\", female_name_gen, female_data, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OWuQx8Urr_Qo",
        "outputId": "c1fe20f3-4bb4-43a2-c9e4-68ff7766d4c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'luna'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons une liste conséquente de prénoms féminins, pour toutes les lettres de l'alphabet."
      ],
      "metadata": {
        "id": "NA412zGe8EVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "female_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_female_names = []\n",
        "  for _ in range(n_gen):\n",
        "    female_name = generate_name(init_letter, female_name_gen,\n",
        "                                female_data, device)\n",
        "    letter_female_names.append(female_name)\n",
        "\n",
        "  female_names.extend(letter_female_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_female_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpF-Pgip-Y2Y",
        "outputId": "bdc04340-c341-4f07-ad2c-1eabcbed4980"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: aulyll, argesta, aumel, anderitte, alicra, andel, alli, argyel, aletra, aurelle\n",
            "b: bianthita, brab, billen, braelie, brilly, baidia, brimtra, bmristine, bimtrina, berjianne\n",
            "c: chella, celiy, cirthele, cavissin, cieh, cathrie, carde, carhelle, cas, cath\n",
            "d: dvarri, demin, dirky, dandi, dess, deflynn, darley, dellie, dwalcathanl, daletia\n",
            "e: ellbie, eusty, elbarina, elorie, elica, elena, etincisa, edette, elma, elobetta\n",
            "f: frenney, faverinie, fyaara, flagie, franmy, fekdia, frandar, fror, frynnie, frandsa\n",
            "g: gome, gony, goridaul, gertia, gickie, gay, gea, gorelei, granni, gunjeentins\n",
            "h: hilys, hally, henetta, halene, hivfrindte, hollica, hemhil, hosfits, helprina, hillane\n",
            "i: ilballata, idellka, iella, ida, imecella, inna, igma, ilisra, ipy, iborta\n",
            "j: jisanph, jonkti, jerista, jore, jopa, julissanne, junor, jeann, jioonna, jeanne\n",
            "k: kast, karlece, ketty, kaulle, keltabe, kardy, kathine, kelli, kags, katbel\n",
            "l: lili, lotrie, laile, lillinne, layby, lorah, lorea, loranna, leacive, lerin\n",
            "m: mlaygea, mangqie, merni, madcie, meran, marmerta, meadonn, margul, marree, myela\n",
            "n: nidy, nagis, neria, nyxy, ninssia, nandu, nic, nanes, ninny, nandie\n",
            "o: olmeria, opleo, obby, osatinta, onamire, onanetta, oilveh, ondya, olanre, odelin\n",
            "p: pathren, pellette, palleen, porth, porigabi, pardote, pellyns, phicy, pasi, pil\n",
            "q: quelajia, quabine, quinnah, quannex, queena, querhy, quannel, qlishella, qomeilrine, quenelle\n",
            "r: robalva, rosie, rlosella, reedeia, ribritta, rozann, rilli, renolend, rovena, ropav\n",
            "s: shya, shir, sun, shacy, shin, shean, sharlene, shati, systin, snella\n",
            "t: tori, tade-lea, tubpida, trofi, tissay, tesfiida, tongia, terice, thily, terri\n",
            "u: unna, uuthorha, uxlianda, ully, umi, uggarlyn, ulisal, uminetta, udofalee, uobel\n",
            "v: velissa, vilena, vortyn, vagkardina, vagrie, va, varinde, violia, vyndean, vibin\n",
            "w: wonnana, wynna, wilisa, wel, winnye, wan, wille, walllie, wen, willabebaria\n",
            "x: xuenita, xilisa, xixela, xonbelle, xanettie, xiy, xaina, xic, xalorine, xoneth\n",
            "y: yliesa, yorina, yeen, ytabetta, yletta, ylethe, yienda, yphine, ylol, yleenda\n",
            "z: zatta, zzesta, zoron, zanci, zetse, ziby, zeora, zel, zare, zera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "XajfC7vu8PtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "male_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_male_names = []\n",
        "  for _ in range(n_gen):\n",
        "    male_name = generate_name(init_letter, male_name_gen,\n",
        "                              male_data, device)\n",
        "    letter_male_names.append(male_name)\n",
        "\n",
        "  male_names.extend(letter_male_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_male_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9LK-nF6-bHb",
        "outputId": "5481a96e-c984-4569-e678-7b0b0ea83253"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: amgrin, avlob, awe, andord, albrik, anled, ake, anrinke, amreys, anfourch\n",
            "b: barrerd, birroch, branol, baiyron, berd, bim, bergie, bevold, byy, berces\n",
            "c: conl, chamofe, chupbaldon, cqamue, chreldie, crarnih, cbeitt, cirryy, cerese, carckar\n",
            "d: dorer, dinterg, dipby, doran, distandy, darmreen, docen, dave, dumrel, daril\n",
            "e: enmaet, eaxulm, efter, eflilno, erdand, elury, ekaal, erakly, evelf, etten\n",
            "f: furfie, fremlon, furech, firlal, fyy, fungian, frestar, famt, farmarlie, frin\n",
            "g: gawue, garstan, goanfee, gametten, gugdeeb, golwrinl, grenom, gaale, gormy, garrinn\n",
            "h: hasthin, hich, heeras, hard, hyy, ho, hilmy, hybon, horrie, hurtaldie\n",
            "i: irbertan, ironho, ilstico, igole, ijhy, iounlis, ilisp, iduci, iscorthorian, itun\n",
            "j: joyras, jeynew, javerosor, jovof, jheuamunn, jance, jerripson, jrandy, jiabe, jaal\n",
            "k: kepkartz, kafvorcop, kacher, kaospy, kas, kan, komasten, khinog, karpos, kavian\n",
            "l: long, likd, lov, labward, lulocil, lent, loydanwon, libersun, lerriv, less\n",
            "m: merak, marmand, merbel, marly, mumola, muanfan, moblyy, michon, minno, marwishen\n",
            "n: noxrien, naokie, nemny, niefian, netaldus, nennerlo, nrett, nones, neun, neymie\n",
            "o: ogbarriih, orn, o, olly, obal, orily, oefsos, onborgo, orcansrik, oshady\n",
            "p: peerfuph, peyik, pachin, penk, paoby, padberton, pandar, pamweem, pronckeucl, peryer\n",
            "q: qaynie, qaalnlers, qakilly, qalgon, qabiys, qeiiqonite, qandiif, qareby, quigkin, qagiit\n",
            "r: redman, rerdene, revastasey, ralen, redras, rub, ragih, rars, rtiodarte, relligi\n",
            "s: shadtarard, sermon, sbobry, seras, silmante, sorriy, shefce, shurdy, smichy, sogdie\n",
            "t: tramrer, taodie, tregas, tarbie, tocties, theonler, tlepees, taxsor, toyne, tinberel\n",
            "u: unnie, ulnelulce, udlaston, umis, unnant, urens, ulosom, ulletdeo, ulicmar, ubkdalce\n",
            "v: vaydoun, vidy, vin, vamius, veowar, veanris, vrastom, vadtico, vicpy, vom\n",
            "w: wanren, wodein, wintichas, wemwennon, wens, wabeny, wods, wav, wrinpe, wonnich\n",
            "x: xille, xadmiol, xifdan, xovo, xalfy, xuchor, xamar, xipie, xammie, xater\n",
            "y: yranlily, yals, yodia, ylnill, yanny, yit, ynewen, ygeran, yeris, yict\n",
            "z: zerabney, zuc-xogoron, zom, zaviy, zork, zobdaule, zanimy, zenig, zikres, zege\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant regarder si notre générateur a créé des prénoms qui se trouvaient dans nos listes. Notre modèle est intéressant s'il crée quelques prénoms existants, mais pas en majorité. S'il ne crée que des prénoms existants, c'est qu'il a appris notre liste par coeur et est beaucoup trop entrainé."
      ],
      "metadata": {
        "id": "ici9YjIL8TNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_found_names = [name for name in female_names\n",
        "                      if name + \"\\n\"*(female_data.max_len - len(name))\n",
        "                      in female_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(female_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCzziOhN-j5B",
        "outputId": "9a68d534-8c04-4fb9-9ed1-3b02cb5fed90"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alli, elena, gay, hally, ida, inna, jeanne, ketty, kelli, rosie, shir,\n",
            "sharlene, tori, terri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_found_names = [name for name in male_names\n",
        "                    if name + \"\\n\"*(male_data.max_len - len(name))\n",
        "                    in male_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(male_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfzmgVUa-mew",
        "outputId": "e1381c6d-8384-4b0d-ee6c-65bc8e92715a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dave, vin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Y8zvxjd0CZfX"
      }
    }
  ]
}