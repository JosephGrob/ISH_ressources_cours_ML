{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TsPR9wc5_70_isxcjOj-QL-eBYwzwjqA",
      "authorship_tag": "ABX9TyNZEBwqzslpe5D44fXhwHMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gguex/ISH_ressources_cours_ML/blob/main/TP11_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP 11 : LSTM et GRU\n",
        "\n",
        "Dans ce TP, nous allons apprendre à créer et entrainer des réseaux de type LSTM et GRU pour créer des modèles génératifs. \n",
        "\n",
        "Dans la première partie, nous allons entrainer un réseau LSTM qui permet de générer du texte \"similaire\" à des données d'entrainement. Nous utiliserons pour cela le premier livre des Misérables.\n",
        "\n",
        "Dans la deuxième partie, nous allons entrainer un réseau GRU qui permet de générer des prénoms féminins ou masculins qui \"sonnent juste\", mais qui n'existent pas nécessairement. Nous utiliserons pour cela le jeu de données de prénoms anglophones trouvé sur http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/ (mais vous pouvez essayer avec d'autres listes de prénoms, ces dernières sont faciles à trouver).\n",
        "\n",
        "Les librairies nécessaires sont les suivantes :"
      ],
      "metadata": {
        "id": "xJftcrf_fBjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqoWHussKYEq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "import string\n",
        "# Permet d'afficher le texte avec une certaine largeur\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enregistrons le dispositif de calcul dans une variable."
      ],
      "metadata": {
        "id": "S8FXdFVtj3Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ksaGk3fkEX-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dbca27-ee29-4e17-a562-27ee12d43b24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Génération de textes avec LSTM\n",
        "\n",
        "Dans cette partie, nous allons créer un modèle de génération de textes qui sera entrainé sur le premier livre des Misérables (cf. TP 10). Ce modèle sera constitué de plusieurs couches LSTM successives, et devra être entrainé à prédire le prochain token d'une séquence en fonction des tokens précédents."
      ],
      "metadata": {
        "id": "eXUpfBThkE4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` pour pouvoir générer des exemples d'entrainement. \n",
        "\n",
        "Un entrée/sortie de ce dataset sera constitué de deux séquences, tirées de notre document, de taille `seq_len`. La séquence de sortie sera décalée d'un token sur la droite par rapport à l'entrée (voir l'exemple plus bas). Ainsi, chaque token de la séquence d'entrée sera associé avec le token suivant.\n",
        "\n",
        "Nous allons utiliser un modèle de langage de Spacy pour tokeniser notre document. "
      ],
      "metadata": {
        "id": "y0ATSP1nku7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceData(Dataset):\n",
        "    def __init__(self, file_path, nlp, seq_len):\n",
        "\n",
        "        # Le chemin d'accès au fichier\n",
        "        self.file_path = file_path\n",
        "        # La longueur des séquences d'entrée et sortie\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # On ouvre notre fichier et on crée un object Spacy\n",
        "        with open(self.file_path, \"r\") as f:\n",
        "          text = f.read()\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # On découpe notre texte en tokens, sans les espaces\n",
        "        self.tokens = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "        # On enregistre le vocabulaire utilisé, et on crée des dictionnaires\n",
        "        # permettant de transformer chaque token en identifiant numérique, \n",
        "        # ou l'inverse.\n",
        "        self.dictionary = list(set(self.tokens))\n",
        "        self.id2token = {id: token for id, token in enumerate(self.dictionary)}\n",
        "        self.token2id = {token: id for id, token in enumerate(self.dictionary)}\n",
        "\n",
        "        # On transforme notre corpus en une séquence de valeurs numériques\n",
        "        self.token_ids = [self.token2id[w] for w in self.tokens]\n",
        "\n",
        "    # La taille de notre dataset est le nombre de séquences possibles\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.seq_len\n",
        "\n",
        "    # Un élément de notre corpus sera la séquence id et la séquence id+1.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.token_ids[id:id+self.seq_len]),\n",
        "                torch.tensor(self.token_ids[id+1:id+self.seq_len+1]))"
      ],
      "metadata": {
        "id": "aBf2WJTKEXEO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On charge le modèle de langage de Spacy."
      ],
      "metadata": {
        "id": "l4tegHb-oKGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !spacy download \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "nX5JqfXwEjyR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre classe `SentenceData`, en utilisant le premier livre de notre corpus et en le divisant en séquences de 10 tokens."
      ],
      "metadata": {
        "id": "HoC3igPr0MuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/book_01.txt\"\n",
        "example_doc = SentenceData(doc_path, nlp, 10)"
      ],
      "metadata": {
        "id": "4vRKSDCwGmnp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre document contient 28932 paires de séquences."
      ],
      "metadata": {
        "id": "51HAOH7u0VD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJMYd_sH1XI",
        "outputId": "2d815fd6-99dc-4422-d7d4-ee14c3764eb5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28932"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut examiner un exemple en particulier, en traduisant les séquences numériques en mots."
      ],
      "metadata": {
        "id": "RX4XIx9v0dpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id = 0\n",
        "input, output = example_doc[sample_id]\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in input]))\n",
        "print(\" \".join([example_doc.id2token[id.item()] for id in output]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBb9EO8XHORw",
        "outputId": "2303e663-46e4-4522-efc6-35eab4f4d471"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 1815 , M. Charles - François - Bienvenu Myriel\n",
            "1815 , M. Charles - François - Bienvenu Myriel was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un objet `torch.utils.data.Dataloader` pour parcourir notre base de données, avec une taille de batch donnée."
      ],
      "metadata": {
        "id": "lXIhKhPm0nPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "my_dataloader = DataLoader(example_doc, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rP1RISBZNbzX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps de créer notre modèle. Ce dernier prendra notre séquence d'entrée et sera consititué de :\n",
        "\n",
        "* Une couche d'embedding, qui transforme nos identifiants numériques en vecteurs one-hot de taille `n_vocab`, puis en vecteur de taille `embedding_dim`.\n",
        "* `num_layers` couches de LSTM, avec des vecteurs d'état caché et d'état de cellule de taille `hidden_size`. On peut ajouter un dropout aux couches, afin que ces dernières n'apprennent pas le texte entièrement par coeur.\n",
        "* Une couche entièrement connectée, avec `n_vocab` sorties.\n",
        "\n",
        "La sortie de ce réseau donnera les log-odds pour les mots suivants, qui peuvent entre converties en probabilités.\n",
        "\n",
        "Notez que la longueur de la séquence n'a pas besoin d'être précisée ici. Pytorch se chargera de prendre tous les éléments reçus, de \"déplier\" le réseau en fonction de leur nombre, et de calculer les sorties en transmettant les états entre les cellules. \n",
        "\n",
        "Remarquez également que nous précisons avec `batch_first=True`, lors de la création des couches LSTM, que notre batch se situe sur la première dimension de nos tenseurs. Ainsi, Pytorch comprend que la séquence se situe sur la deuxième dimension des tenseurs (voir https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)."
      ],
      "metadata": {
        "id": "cm42-xGY1GmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqGen(nn.Module):\n",
        "    def __init__(self, n_vocab, embedding_dim, hidden_size, num_layers):\n",
        "        super(SeqGen, self).__init__()\n",
        "\n",
        "        # On sauve les paramètres dans des attributs\n",
        "        self.n_vocab = n_vocab\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # La couche d'embedding\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=self.n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        # Les couches LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    # La fonction foward peut prendre, en plus des entrées,\n",
        "    # l'état caché et l'état de la cellule utilisé au début de la séquence.\n",
        "    # Par défaut, ces états sont posés comme None.\n",
        "    def forward(self, x, prev_state=None):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "kj65kz_oKnr6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une instance de notre modèle, avec nombre de mots correspondant à notre vocabulaire, embedding de 128 et 3 couches de LSTM avec états de taille 128."
      ],
      "metadata": {
        "id": "-YhjGZK7_5PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_gen = SeqGen(len(example_doc.dictionary), 128, 128, 3)"
      ],
      "metadata": {
        "id": "bcnKUn8oTO6e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser l'entropie croisée et l'optimisateur Adam pour entrainer notre modèle."
      ],
      "metadata": {
        "id": "LgnEYlT5AQTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(seq_gen.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jPcpxj7OT4VA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On entraine maintenant le modèle. Notez qu'aucun état caché est donné à notre modèle avant chaque batch, ce qui veut dire que les états initiaux seront nuls avant chaque séquence. L'entrainement est relativement simple, car la surparamétrisation n'est pas très importante pour notre modèle."
      ],
      "metadata": {
        "id": "B2581VwZAcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le nombre d'epochs\n",
        "n_epochs = 30\n",
        "\n",
        "# On met le modèle sur le dispositif de calcul \n",
        "seq_gen.to(device)\n",
        "# On le met en mode entrainement\n",
        "seq_gen.train()\n",
        "# On boucle sur les epochs\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "  # Pour calculer la perte moyenne\n",
        "  sum_loss = 0\n",
        "  # On boucle sur notre dataloader\n",
        "  for input, output in my_dataloader:\n",
        "\n",
        "    # Les entrées et sorties sont mises sur le dispositif de calcul\n",
        "    input = input.to(device)\n",
        "    output = output.to(device)\n",
        "\n",
        "    # On met à zéro les gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # On fait les prédictions\n",
        "    pred, _ = seq_gen(input)\n",
        "    # On calcule la perte, en transposant nos résultats\n",
        "    loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "    # On fait une itération de descente du gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # On ajoute la perte\n",
        "    sum_loss += loss.item()\n",
        "\n",
        "  print(f\"mean loss = {sum_loss / len(my_dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWiGXq-T7ty",
        "outputId": "aae4bacf-6dbc-41ba-ec8c-8c4cc7d60885"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean loss = 6.7448\n",
            "Epoch 2: mean loss = 6.2605\n",
            "Epoch 3: mean loss = 6.1509\n",
            "Epoch 4: mean loss = 6.0067\n",
            "Epoch 5: mean loss = 5.8053\n",
            "Epoch 6: mean loss = 5.6047\n",
            "Epoch 7: mean loss = 5.4250\n",
            "Epoch 8: mean loss = 5.2537\n",
            "Epoch 9: mean loss = 5.0827\n",
            "Epoch 10: mean loss = 4.9274\n",
            "Epoch 11: mean loss = 4.7922\n",
            "Epoch 12: mean loss = 4.6605\n",
            "Epoch 13: mean loss = 4.5398\n",
            "Epoch 14: mean loss = 4.4297\n",
            "Epoch 15: mean loss = 4.3234\n",
            "Epoch 16: mean loss = 4.2263\n",
            "Epoch 17: mean loss = 4.1343\n",
            "Epoch 18: mean loss = 4.0511\n",
            "Epoch 19: mean loss = 3.9731\n",
            "Epoch 20: mean loss = 3.8962\n",
            "Epoch 21: mean loss = 3.8219\n",
            "Epoch 22: mean loss = 3.7543\n",
            "Epoch 23: mean loss = 3.6838\n",
            "Epoch 24: mean loss = 3.6146\n",
            "Epoch 25: mean loss = 3.5455\n",
            "Epoch 26: mean loss = 3.4790\n",
            "Epoch 27: mean loss = 3.4170\n",
            "Epoch 28: mean loss = 3.3507\n",
            "Epoch 29: mean loss = 3.2866\n",
            "Epoch 30: mean loss = 3.2233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant donner à notre modèle un début de séquence et voir comment il génère une suite. Le processus se passe en deux étapes :\n",
        "\n",
        "* On passe une première fois la séquence initiale dans notre modèle, afin de générer le prochain token et pour récupérer l'état caché résultant.\n",
        "* On va ensuite faire une boucle pour les tokens restants, en passant dans le réseau le dernier token et états obtenus à l'étape précédente.\n",
        "\n",
        "Pour générer chaque nouveau token, on transforme les log-odds en probabilités, puis on effectue un tirage aléatoire selon ces dernières.\n",
        "\n",
        "Le séquence finale est affichée formattée."
      ],
      "metadata": {
        "id": "P2pM7sEdGYaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Séquence initiale\n",
        "input_sentence = \"When the man\"\n",
        "# Nombre de tokens désiré\n",
        "n_generated_tokens = 600\n",
        "\n",
        "# --- Etape 1\n",
        "\n",
        "# On coupe notre séquence intiale pour créer notre séquence de sortie\n",
        "output_tokens = input_sentence.split()\n",
        "# On met le modèle en mode évaluation\n",
        "seq_gen.eval()\n",
        "# Notre séquence d'entrée est transformée en identifiants numériques\n",
        "input = torch.tensor([example_doc.token2id[token] \n",
        "                      for token in output_tokens]).to(device)\n",
        "# On passe nos entrées dans notre modèle, en ne donnant aucun état.\n",
        "pred, hidden = seq_gen(input)\n",
        "# On garde les logodds du dernier mot. Notez que nous utilisons \n",
        "# detach().cpu() pour ne pas garder le graph de calcul et pour \n",
        "# basculer le tenseur sur le cpu.\n",
        "new_token_logodds = pred.detach().cpu()[-1]\n",
        "# Les probabilités sont calculées avec un softmax\n",
        "probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "# On tire aléatoirement le token suivant, avec les probabilités précédentes\n",
        "token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "# On l'ajoute à notre sortie\n",
        "output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# --- Etape 2\n",
        "\n",
        "# On boucle sur le nombre restant de tokens demandés\n",
        "for i in range(n_generated_tokens - 1):\n",
        "  # Notre dernier token est transformé en entrée\n",
        "  input = torch.tensor([example_doc.token2id[output_tokens[-1]]]).to(device)\n",
        "  # On passe notre entrée, avec les états précédants, dans notre modèle\n",
        "  pred, hidden = seq_gen(input, hidden)\n",
        "  # On détache, met sur le cpu et applatit la sortie\n",
        "  new_token_logodds = pred.detach().cpu().flatten()\n",
        "  # Les probabilités sont calculées avec un softmax\n",
        "  probs = torch.nn.functional.softmax(new_token_logodds, dim=0).numpy()\n",
        "  # On tire aléatoirement, avec les probabilités calculées, le token suivant\n",
        "  token_index = np.random.choice(len(new_token_logodds), p=probs)\n",
        "  # On l'ajoute à notre sortie\n",
        "  output_tokens.append(example_doc.id2token[token_index])\n",
        "\n",
        "# On affiche notre sortie formatée\n",
        "print(textwrap.fill(\" \".join(output_tokens), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0JzgJA9VfIk",
        "outputId": "50d32b08-5f4e-43f1-e26a-5a6dd706671e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the man was completed in that or an hundred door by perfection . These\n",
            "would go for the service of her seized and a carriage , along lean the see of D\n",
            "— — wear it to a bishop a court of possess one draw at the conscience of the\n",
            "Faculty of Honor dazzles They have already been well they thought alone by\n",
            "peril ; we have put as tolerant in God , adoring by risking complete ; hence de\n",
            "one who put them . Among Majesty called without poor as quite priests with\n",
            "solicit him from an existence and Italy ! He would not hear his hour of ducks .\n",
            "Happy he would scent asleep to entertain him than overflowed men , who have\n",
            "come to indicate for a furnace , and has an single long old - sightedness . No\n",
            "persons are so body in questions which more were twenty towards that Cravatte\n",
            "erect ; which whispers with him than opening ; this Myriel is a place and Italy\n",
            "; ” he had what the worm of this senses of exactness in success , without its\n",
            "drawing , adoring of tacitly clapping which engross mountebank , while who\n",
            "admit ; the other waited to blaze up in the subject of progress incarcerated\n",
            "for burst ; ‘ hold on its belief and pure harshness , and behold himself , as\n",
            "far when Prussia the magician asperity , is ability to admired for this ; he\n",
            "how formed as we like a pork of threadbare grand coteries : he was vaguely by\n",
            "addressed with that infantile other still drawl ! There are confessed on any\n",
            "antechamber , as he was fond of a powers , with all summoned by Aix . ”—_“Well\n",
            ", it is not appointed them ! ” For maternity an smile , and mounts tone ; but\n",
            "Such would be so found Madame is irresponsible , Baron ; we will not keep finds\n",
            "ourselves at his single notable livres which off distinctly on the “ court of\n",
            "residence for his one was an city , thin to solve that you do not wish to\n",
            "comprehend the shadow , and acclamation , in meditate ; Baron arrived not\n",
            "martyrs of huge presented avoided , it he anything at our charms , then prêtres\n",
            "like an abode . Every round ; he gazed out and so gravity with the armpits and\n",
            "assumed their wants on prayer , and mounts must , possibly , a whole goodman of\n",
            "watching anything , we can not appear to be counterfeit district for digs\n",
            "bearings . Is he Brignolles - cold ; ’ fifteen might have been heretics . And\n",
            "he shut to his senses , softened much tenderness in retiring , as we think\n",
            "ought a duty and packer immense hand ? “ Was the hospital , ” said the horizon\n",
            "— gardening as a face who are before occupies , taking asking perhaps . What\n",
            "was a palimpsest , which Lucretius , Dante , certainly softened by exclamation\n",
            "anything . Be him by those nails , and growing insist on the steward of the\n",
            "beginning and full of truths . Every absolved of stature on the shepherd ;\n",
            "wears the world survived : it 1806 ! ” I tore his wants in senator . He would\n",
            "us think that which are wish to selected in this bishop , sold softened by\n",
            "precipitation forth them . He considered it seems as it analyzes in an\n",
            "cardinalship , on sold with reveal against his thought , after it , and\n",
            "acclamation , while him boiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "weevWLMZ9bQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Génération de prénoms avec GRU\n",
        "\n",
        "Maintenant, nous allons utiliser les GRU pour créer des modèles de génération de prénoms, en utilisant des données de prénoms féminins et masculins. Le principe reste assez similaire à la partie précédente, mais nous adopterons une programmation de style plus fonctionnel."
      ],
      "metadata": {
        "id": "_zoqTHMJLtZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par créer une classe héritée de `torch.utils.data.Dataset` qui va donner des exemples constitués d'un nom comme entrée et du même nom décalé d'une lettre sur la droite comme sortie. Comme caractère de fin de séquence et de padding, nous allons utiliser le caractère `\"\\n\"` déjà présent à la fin de chaque nom dans le fichier (sauf du dernier). Nous effectuons un padding sur tous les prénoms afin que tous fassent la taille du plus long prénom (pour entrainer les modèles avec des batchs)."
      ],
      "metadata": {
        "id": "qPy0pb8TMCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameData(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        # On enregistre le chemin du fichier\n",
        "        self.file_path = file_path\n",
        "        # On ouvre le fichier et on lit les lignes\n",
        "        with open(file_path, \"r\") as f:\n",
        "          self.lines = f.readlines()\n",
        "        # On ajoute notre caractère de padding à la dernière ligne\n",
        "        self.lines[-1] += \"\\n\"\n",
        "        # La longueur maximale est la longueur de la plus grande ligne\n",
        "        self.max_len = max([len(line) for line in self.lines])\n",
        "        # Les lignes sont complétées avec \"\\n\" pour avoir la même taille\n",
        "        self.lines = [line + \"\\n\"*(self.max_len - len(line)) \n",
        "                      for line in self.lines]\n",
        "        # On met les lignes en minuscules\n",
        "        self.lines_low = [line.lower() for line in self.lines]\n",
        "        # On sauve le nombre de lettres différentes\n",
        "        self.letters = list(set(\"\".join(self.lines_low)))\n",
        "        # On crée notre liste de nom, avec chaque lettre \n",
        "        # comme élément d'une liste\n",
        "        self.names = [list(line) for line in self.lines_low]\n",
        "\n",
        "        # Ces fonctions permettent de transformer les lettres en identifiants \n",
        "        # numériques, ou vice-versa.\n",
        "        self.id2letter = {id: letter for id, letter in enumerate(self.letters)}\n",
        "        self.letter2id = {letter: id for id, letter in enumerate(self.letters)}\n",
        "\n",
        "        # On sauve l'identifiant du caractère de padding \n",
        "        self.endl_id = self.letter2id[\"\\n\"]\n",
        "\n",
        "        # On transforme nos listes de lettres en listes d'ids\n",
        "        self.names_ids = [[self.letter2id[l] for l in name] \n",
        "                          for name in self.names]\n",
        "\n",
        "    # La longueur du dataset\n",
        "    def __len__(self):\n",
        "        return len(self.names_ids)\n",
        "\n",
        "    # Un item est une entrée/sortie avec, respectivement, \n",
        "    # prénom sans la dernière lettre, prénom sans la première lettre.\n",
        "    def __getitem__(self, id):\n",
        "        return (torch.tensor(self.names_ids[id][:-1]),\n",
        "                torch.tensor(self.names_ids[id][1:]))"
      ],
      "metadata": {
        "id": "2PLVeGU39cC4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée maintenant nos instances contenant nos jeux de données. "
      ],
      "metadata": {
        "id": "BpFQh1DkOrJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/female.txt\"\n",
        "male_path = \"/content/drive/MyDrive/Colab Notebooks/ml_data/TP11/male.txt\"\n",
        "female_data = NameData(female_path)\n",
        "male_data = NameData(male_path)"
      ],
      "metadata": {
        "id": "ukgFs2NC9yMT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons un exemple."
      ],
      "metadata": {
        "id": "9OLmhvMiO0ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, output = female_data[0]\n",
        "print([female_data.id2letter[id.item()] for id in input])\n",
        "print([female_data.id2letter[id.item()] for id in output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYDL5IPb94bZ",
        "outputId": "2bc85f54-6820-4ab7-8f39-e0e6646b93c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n",
            "['b', 'a', 'g', 'a', 'e', 'l', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous créons maintenant la classe de notre modèle. Ce dernier passe  directement les vecteurs one-hot dans les couches GRU, car le nombre de caractères existants n'est pas très élevé (la dimensionnalité des vecteurs one-hot est déjà basse). Le modèle est constitué de :\n",
        "\n",
        "* `num_layer` couches GRU avec états cachés de taille `hidden_size`.\n",
        "* Une couche entièrement connectée qui envoie les états cachés finaux sur la taille du vocabulaire.\n",
        "\n",
        "Le résultat du modèle sont les log-odds des prochains caractères."
      ],
      "metadata": {
        "id": "XE6CYho4jfOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameGen(nn.Module):\n",
        "    def __init__(self, n_vocab, hidden_size, num_layers):\n",
        "        super(NameGen, self).__init__()\n",
        "\n",
        "        # On enregistre les paramètres du modèle\n",
        "        self.n_vocab = n_vocab\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Les couches GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.n_vocab,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # La couche entièrement connectée\n",
        "        self.lin_layer = nn.Linear(self.hidden_size, self.n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        # Cette fonction transforme les entiers en vecteurs one-hot\n",
        "        one_hot = nn.functional.one_hot(x, num_classes=self.n_vocab)\n",
        "        # On passe dans les couches GRU\n",
        "        output, state = self.gru(one_hot.to(torch.float), prev_state)\n",
        "        # Les log-odds résultantes\n",
        "        logodds = self.lin_layer(output)\n",
        "        return logodds, state"
      ],
      "metadata": {
        "id": "zQ0fLqLc-Eab"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous instancions un modèle et lui passons un exemple, pour voir si tout fonctionne."
      ],
      "metadata": {
        "id": "ZjaDToc6lQWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 20, 2)\n",
        "female_name_gen(input)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV5nAhek-FDs",
        "outputId": "32cfc47e-318f-4c51-d7ad-1d1a59abab7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'idée de prendre une approche fonctionnelle, nous définissons ici la fonction qui permet d'entrainer un modèle."
      ],
      "metadata": {
        "id": "idkOpfztle6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, batch_size, n_epochs, device):\n",
        "\n",
        "  # On définit la fonction de perte et l'optimisateur\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  # On crée le dataloader\n",
        "  dataloader = DataLoader(data, batch_size, shuffle=True)\n",
        "\n",
        "  # On bascule le modèle en mode entrainement\n",
        "  model.train()\n",
        "  # On le met sur le dispositif de calcul\n",
        "  model.to(device)\n",
        "  # La boucle d'entrainement\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\", end=\": \")\n",
        "\n",
        "    # Pour calculer la perte moyenne\n",
        "    sum_loss = 0\n",
        "    # La boucle sur les batchs\n",
        "    for input, output in dataloader:\n",
        "\n",
        "      # On met l'entrée et la sortie sur le dispositif de calcul\n",
        "      input = input.to(device)\n",
        "      output = output.to(device)\n",
        "\n",
        "      # On met les gradients à zéro, on fait des prédictions et on\n",
        "      # calcule la perte\n",
        "      optimizer.zero_grad()\n",
        "      pred, _ = model(input)\n",
        "      loss = loss_fn(pred.transpose(1, 2), output)\n",
        "\n",
        "      # On effectue la descente du gradient et on cumule les pertes\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      sum_loss += loss.item()\n",
        "\n",
        "    print(f\"mean Loss = {sum_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "s1xFZYsw-L1S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise maintenant la fonction créée pour entrainer un modèle avec les prénoms féminins."
      ],
      "metadata": {
        "id": "WU9xzgalmgBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_name_gen = NameGen(len(female_data.letters), 30, 3)\n",
        "train_model(female_name_gen, female_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUUZldXv-Q08",
        "outputId": "e72e9eee-9580-41b3-d235-8d50bdeacaa0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.3533\n",
            "Epoch 2: mean Loss = 0.9808\n",
            "Epoch 3: mean Loss = 0.9200\n",
            "Epoch 4: mean Loss = 0.8827\n",
            "Epoch 5: mean Loss = 0.8650\n",
            "Epoch 6: mean Loss = 0.8524\n",
            "Epoch 7: mean Loss = 0.8406\n",
            "Epoch 8: mean Loss = 0.8288\n",
            "Epoch 9: mean Loss = 0.8169\n",
            "Epoch 10: mean Loss = 0.8051\n",
            "Epoch 11: mean Loss = 0.7922\n",
            "Epoch 12: mean Loss = 0.7764\n",
            "Epoch 13: mean Loss = 0.7635\n",
            "Epoch 14: mean Loss = 0.7533\n",
            "Epoch 15: mean Loss = 0.7452\n",
            "Epoch 16: mean Loss = 0.7384\n",
            "Epoch 17: mean Loss = 0.7318\n",
            "Epoch 18: mean Loss = 0.7259\n",
            "Epoch 19: mean Loss = 0.7208\n",
            "Epoch 20: mean Loss = 0.7147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "eg4j_PXPmpOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_name_gen = NameGen(len(male_data.letters), 30, 3)\n",
        "train_model(male_name_gen, male_data, 16, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PthBaFgw-TIH",
        "outputId": "ed55b78d-422b-4edd-ee38-45f92be6a6e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: mean Loss = 1.5407\n",
            "Epoch 2: mean Loss = 1.1193\n",
            "Epoch 3: mean Loss = 1.0523\n",
            "Epoch 4: mean Loss = 1.0283\n",
            "Epoch 5: mean Loss = 1.0116\n",
            "Epoch 6: mean Loss = 0.9865\n",
            "Epoch 7: mean Loss = 0.9443\n",
            "Epoch 8: mean Loss = 0.9230\n",
            "Epoch 9: mean Loss = 0.9117\n",
            "Epoch 10: mean Loss = 0.9026\n",
            "Epoch 11: mean Loss = 0.8943\n",
            "Epoch 12: mean Loss = 0.8844\n",
            "Epoch 13: mean Loss = 0.8752\n",
            "Epoch 14: mean Loss = 0.8663\n",
            "Epoch 15: mean Loss = 0.8599\n",
            "Epoch 16: mean Loss = 0.8523\n",
            "Epoch 17: mean Loss = 0.8464\n",
            "Epoch 18: mean Loss = 0.8399\n",
            "Epoch 19: mean Loss = 0.8337\n",
            "Epoch 20: mean Loss = 0.8279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons maintenant une fonction qui permet de générer un prénom en fonction des premiers caractères. La procédure est très similaire à la génération de tokens, mais on va arrêter la génération dès que notre modèle prédit le caractère \"\\n\", signifiant la fin de séquence."
      ],
      "metadata": {
        "id": "sepYp7C9snFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_name(init_letters, model, data, device):\n",
        "\n",
        "  # --- Etape 1\n",
        "\n",
        "  output_letters = list(init_letters)\n",
        "  model.eval()\n",
        "  input = torch.tensor([data.letter2id[letter] \n",
        "                        for letter in output_letters]).to(device)\n",
        "  pred, hidden = model(input)\n",
        "  new_letter_logodds = pred.detach().cpu()[-1]\n",
        "  probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "  next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "  output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  # --- Etape 2\n",
        "\n",
        "  # On boucle tant que l'on a pas généré \"\\n\"\n",
        "  while not output_letters[-1] == \"\\n\":\n",
        "    input = torch.tensor([data.letter2id[output_letters[-1]]]).to(device)\n",
        "    pred, hidden = model(input, hidden)\n",
        "    new_letter_logodds = pred.detach().cpu().flatten()\n",
        "    probs = torch.nn.functional.softmax(new_letter_logodds, dim=0).numpy()\n",
        "    next_letter_id = np.random.choice(len(new_letter_logodds), p=probs)\n",
        "    output_letters.append(data.id2letter[next_letter_id])\n",
        "\n",
        "  return \"\".join(output_letters)[:-1]"
      ],
      "metadata": {
        "id": "m_YmXApw-WpE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testons la fonction de génération avec un début de prénom."
      ],
      "metadata": {
        "id": "X_cRCEZB72vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_name(\"lun\", female_name_gen, female_data, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OWuQx8Urr_Qo",
        "outputId": "99a140ed-c782-454f-8049-d0f2f3e4a243"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lunelie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons une liste conséquente de prénoms féminins, pour toutes les lettres de l'alphabet."
      ],
      "metadata": {
        "id": "NA412zGe8EVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "female_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_female_names = []\n",
        "  for _ in range(n_gen):\n",
        "    female_name = generate_name(init_letter, female_name_gen, \n",
        "                                female_data, device)\n",
        "    letter_female_names.append(female_name)\n",
        "\n",
        "  female_names.extend(letter_female_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_female_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpF-Pgip-Y2Y",
        "outputId": "c18010fb-232f-4854-9ebe-348a4bc597d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: annola, abrina, alenette, ahella, arefia, alwa, alenia, awna, arian, atrittia\n",
            "b: brisa, belalia, bermy, benda, berbenet, binett, bilacoy, brestaia, blindy, bri\n",
            "c: cecaine, celle, currias, calinna, chlesnelle, catha, carmapine, cmarlia, canda, caron\n",
            "d: dekdelle, deesh, doe, dorl, deannetta, dath, dilda, daitha, donina, dinah\n",
            "e: elrora, eybrith, essy, enadetone, endish, egythe, enni-ortalo, emalida, evianna, elesta\n",
            "f: fixine, farria, frit, fraria, farlie, flidista, fordyr, frossita, freltinra, filla\n",
            "g: gwinthaa, gint, genca, gregette, gesaci, greet, geanna, gebbona, gresseh, geoadyln\n",
            "h: hilelin, handretue, harmela, halma, hanni, hannaja, helian, hi, haleana, helitte\n",
            "i: idma, ivila, issopa, imtei, irmenan, inpa, ilma, inen, ivke-tina, idelie\n",
            "j: jecy, jiylel, jodissa, jebena, jliala, jolilet, jannia, jynne, jeine, jindia\n",
            "k: kamelvy, karie, kamorten, kendi, kossi, kriesta, karie, korly, kate, koli\n",
            "l: lorisa, lobora, loruci, lora, liudrita, lana, lauosa, logisha, lerpoe, loishe\n",
            "m: maroloe, mers, morshelle, malye, mayda, medisne, mringia, menellne, mari, mery\n",
            "n: nanlitha, nina, nitha, nie, nadey, nimeith, nyiliy, nansa, noesan, nami\n",
            "o: ota, oelina, otheiba, orel, oda, olli, ogella, ongita, ondabete, onnel\n",
            "p: pensye, pulieva, phlratoa, pormee, peleen, pela, pamonda, pomki, pinny, pilitda\n",
            "q: quendi, quendana, quevry, quiancike, quine, quadie, quistaile, qle, qusolen, qleda\n",
            "r: redy, ricya, ruquel, rucie, roxie, robia, rose, rasayte, racy, rabal\n",
            "s: ssalia, shey, soryne, samy, sira, sylise, sharla, sicivis, somkie, shanin\n",
            "t: tatvah, tuane, toth, trincy, tecer, tari, terra, trete, tacesa, thida\n",
            "u: ustie, umotine, ussi, uejoe, uora, uuline, ulbretha, ulv, ulra, ulessa\n",
            "v: valla, vallice, vrica, vrinnie, vien, velllen, valmia, vimaggha, vynie, vybina\n",
            "w: wmabyl, willia, wray, winnolenta, wermatida, willia, waz, wyna, wela, wamli\n",
            "x: ximeys, xatmie, xistan, xelly, xinny, xuendy, xann, xina, xinbel, xante\n",
            "y: yrine, yolana, ylanne, y, ylistlene, yart, yleona, yalra, ylenna, yledyn\n",
            "z: zora, zanida, zitkte, zalis, zellia, zona, zarona, zoretta, zaphelia, zarci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idem pour les prénoms masculins."
      ],
      "metadata": {
        "id": "XajfC7vu8PtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_letters = string.ascii_lowercase\n",
        "n_gen = 10\n",
        "male_names = []\n",
        "for init_letter in init_letters:\n",
        "  letter_male_names = []\n",
        "  for _ in range(n_gen):\n",
        "    male_name = generate_name(init_letter, male_name_gen, \n",
        "                              male_data, device)\n",
        "    letter_male_names.append(male_name)\n",
        "\n",
        "  male_names.extend(letter_male_names)\n",
        "  print(f\"{init_letter}: {', '.join(letter_male_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9LK-nF6-bHb",
        "outputId": "c966d20f-053b-4814-c2f4-b857c4c3c583"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: alrin, avan, asie, angy, aldom, arwenn, ald, armand, abie, adan\n",
            "b: brad, brico, balad, birlatco, biwle, beddry, bilnoy, ber, bilgi, bilg\n",
            "c: cheoderic-, chitheln, celaul, cerelel, choern, colwin, cim, cleyneld, crisvy, chorris\n",
            "d: deiret, dador, drih, dlod, driskine, dustan, darmah, dreaste, darde, dbardle\n",
            "e: evy, eyner, erryce, engedy, eliull, ebdrick, erd, eplees, eabin, ey\n",
            "f: fyesteje, fripiy, fuger, frecton, froast, filry, fmyne, fame, fope, feirgor\n",
            "g: gunfalie, grayle, gevard, gusap, gore, gastptome, gamo, gerton, gricty, gadare\n",
            "h: hucmarie, hennoind, hrayd, hentiee, horeg, hever, hercy, horme, herech, hay\n",
            "i: idart, ibledste, ictald, ilardm, iath, ihor, irew, idond, imoni, ild\n",
            "j: jiel, jastom, joim, jenwier, jald, jig, jere, juefrella, jeth, jedasaus\n",
            "k: kab, kerster, kevhlerd, kalverd, kanmau, kabily, kastole, kers, koleston, koojie\n",
            "l: lele, lily, lincle, luntre, loanor, landart, lalct, lonm, lerily, loss\n",
            "m: morc, moy, maustlel, mlab, mulguy, mentotan, mub, mether, mim, madfol\n",
            "n: nonk, nadie, numis, nallel, noff, nerr, nove, nynlon, nercho, narre\n",
            "o: ohi, ono, orey, oustie, oulun, orolmis, ozle, other, ownen, ozron\n",
            "p: phart, pirith, parongei, porensis, perip, porburd, pamiro, pandas, puwand, pon\n",
            "q: qiawne, qionmie, qeerto, qaffary, qarf, qiyny, quertin, qiabmy, qonnth, querus\n",
            "r: rase, raymicie, rinti, radule, reby, relt, renk, rhonl, ralge, rocaus\n",
            "s: shantoe, stalmardt, shitan, sitleo, soebin, shonre, shiltoth, szolbeus, svonnol, skajie\n",
            "t: tasalnel, tonker, torn, terty, tewisin, tellel, tredle, titbphals, tpofie, tishar\n",
            "u: umes, urus, unel, uly, ulragp, undem, uger, ulle, ugilbom, udth\n",
            "v: vuwvoy, vibby, virons, vanus, vernce, vasbhin, vear, varn, velan, vrildfe\n",
            "w: wigne, werdert, wayne, wul, wolner, wainie, wednen, worte, weaffy, waud\n",
            "x: xirmilt, xaraly, xeid, xrait, xunlih, x, xanlase, xanlus, xoom, xhanneran\n",
            "y: ylow, yard, ynods, yen, ywyn, yo, yzieit, yer, ylici, ylitm\n",
            "z: zalos, zeo, zelfein, zy, zalii, zhint, zon, zliy, zalrsan, zucorhpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant regarder si notre générateur a créé des prénoms qui se trouvaient dans nos listes. Notre modèle est intéressant s'il crée quelques prénoms existants, mais pas en majorité. S'il ne crée que des prénoms existants, c'est qu'il a appris notre liste par coeur et est beaucoup trop entrainé."
      ],
      "metadata": {
        "id": "ici9YjIL8TNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_found_names = [name for name in female_names \n",
        "                      if name + \"\\n\"*(female_data.max_len - len(name)) \n",
        "                      in female_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(female_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCzziOhN-j5B",
        "outputId": "1529b3df-f0d6-40dc-8974-9347e6f14bf4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "celle, catha, caron, doe, dinah, essy, hanni, karie, karie, kate, lora, lana,\n",
            "mayda, mari, nina, orel, roxie, rose, sharla, terra, zora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_found_names = [name for name in male_names \n",
        "                    if name + \"\\n\"*(male_data.max_len - len(name)) \n",
        "                    in male_data.lines_low]\n",
        "print(textwrap.fill(\", \".join(male_found_names), 79))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfzmgVUa-mew",
        "outputId": "a8e9689c-a79a-4cfb-dcd5-24be7f881683"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "armand, brad, jere, jeth, wayne, yard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Y8zvxjd0CZfX"
      }
    }
  ]
}
